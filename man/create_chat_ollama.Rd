% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/service-ollama.R
\name{create_chat_ollama}
\alias{create_chat_ollama}
\title{Generate text using Ollama's API}
\usage{
create_chat_ollama(
  prompt = list(list(role = "user", content = "Hello")),
  model = "llama3.1:latest",
  api_url = Sys.getenv("OLLAMA_HOST", "http://localhost:11434"),
  stream = FALSE,
  shiny_session = NULL,
  user_prompt = NULL
)
}
\arguments{
\item{prompt}{A list of messages to use as the prompt for generating completions.
Each message should be a list with 'role' and 'content' elements.}

\item{model}{A character string for the model to use.}

\item{api_url}{A character string for the API url. It defaults to the Ollama
host from environment variables or "http://localhost:11434" if not specified.}

\item{stream}{Whether to stream the response, defaults to FALSE.}

\item{shiny_session}{A Shiny session object to send messages to the client}

\item{user_prompt}{A user prompt to send to the client}
}
\value{
The generated completion as a character string, or the full response if streaming.
}
\description{
Use this function to generate text completions using Ollama's API.
}
